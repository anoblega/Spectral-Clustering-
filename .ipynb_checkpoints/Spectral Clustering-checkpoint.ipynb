{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Spectral Clustering\n",
    "\n",
    "\n",
    "## 1. Grafo de Similitud\n",
    "\n",
    "Dados:\n",
    "\n",
    "* Un conjunto de datos $X = \\{x_1, x_2,...,x_n\\}$ \n",
    "\n",
    "* Un valor de similitud $s_{ij} \\geq 0$ entre dos datos $x_i$ y $x_j$. \n",
    "\n",
    "El objetivo es dividir el conjunto $X$ en diferentes grupos, de tal manera que los datos que se encuentran en un mismo grupo son aquellos que son similares entre sí, y diferentes a los que se encuentran en otros grupos. \n",
    "\n",
    "Una manera de representar los *data points* conociendo las *similitudes* entre ellos, es usando una *Matriz de Similitud G = (V,E)*, donde:\n",
    "\n",
    "* Cada vértice $v_i$ representa a un dato $x_i$.\n",
    "\n",
    "* Los vértices $v_i$ y $v_j$ están conectados si su similitud $s_{ij}$ es positiva o mayor a un *threshold*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Notación del Grafo\n",
    "Dado $G = (V, E)$ un grado no-dirigido con el conjunto de vértices $V = \\{v_1, v_2, ..., v_n\\}$.  Asumimos que:\n",
    "\n",
    "\n",
    "![alternate text](pictures/grafo.png)\n",
    "\n",
    "* El grafo $G$ es ponderado, es decir, cada arista entre dos vértices $v_i$ y $v_j$  tiene un peso no-negativo $w_{ij} \\geq 0 $.\n",
    "\n",
    "* La **matriz de adyacencia ponderada**  del grafo es la matriz $W = (w_{ij})_{ij=1,...,n}$ .\n",
    "\n",
    "* Si los vértices $v_i$ y $v_j$ no están conectados entonces $w_{ij} = 0$.\n",
    "\n",
    "* La matriz de adyacencia es *simétrica* pues $w_{ij} = w_{ji}$. \n",
    "\n",
    "* El **grado** de un vértice $v_i$ se define como $$d_i = \\sum_{j = 1}^{n}{w_{ij}}$$ \n",
    "\n",
    "* La **Matriz de Grado D** está definida como la matriz diagonal  con los grados $d_1, d_2,...,d_n$ en la diagonal. \n",
    "\n",
    "![alternate text](pictures/matriz.png)\n",
    "\n",
    "Sea el subconjunto $A \\subset V$ de un **grafo es conexo** si para cualquier par de vértices en $A$ tiene una trayectoria con vértices intermediarios que pertenecen también a $A$.\n",
    "\n",
    "El subconjunto $A$ es llamada **componente conexa** si además de conexo, no hay conexión entre los vértices de $A$ y su complemento $\\bar{A}$.\n",
    "\n",
    "Conociendo lo anterior, podemos definir los conjuntos no vacíos $A_1, A_2, ..., A_k$ son particiones del grafo $A$, tal que:\n",
    "\n",
    "* $A_i \\cap A_j = \\emptyset$\n",
    "* $A_1 \\cup A_2 \\cup ... \\cup A_k = V$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Ejemplos Grafos de Similitud\n",
    "\n",
    "Para transformar un conjunto de *data points* $x_1, x_2, ..., x_n$ en un grafo de similitudes $s_{ij}$ o de distancias $d_{ij}$, existen diversos métodos.\n",
    "\n",
    "Construir *grafos de similitud* ayuda a modelar las relaciones de vecindad local entre los *data points*.\n",
    "\n",
    "* **El Grafo $\\varepsilon-$Neighborhood:**\n",
    "Se conectan aquellos pares de vértices cuya distancia sea más pequeña que $\\varepsilon$. Es así que los pesos del grafo estan en la misma escala.\n",
    "\n",
    "* **El Grafo $k-$Nearest Neighbor**: Los véstices $v_i$ y $v_j$ están conectados si $v_j$ pertenece a los $k$ vecinos más cercanos de $v_i$. Esto define un grafo dirigido, pero ello puede ser solucionado de dos maneras:\n",
    "    * Ignorar las direcciones de los arcos, es decir, existe la arista $e_{ij} \\in E$ tal que $v_i$ es un $k$-vecino más cercano de $v_j$ o viceversa. De ello se obtiene el **grado de k-vecinos más cercanos**\n",
    "    \n",
    "    * Conectar los vértices $v_i$ y $v_j$ si uno pertenece a los $k-$vecinos más cercanos del otro y viceversa, obteniendo el **grafo de k-vecinos más cercanos mutuo**\n",
    "\n",
    "\n",
    "* **El Grafo Completamente Conectado:** Se conectan aquellos vértices cuya similitud es positiva siendo ello su peso de la arista que los une $s_{ij}$. La función a usar debe reflejar las relaciones de vecindad local. \n",
    "\n",
    "    * Un ejemplo de esta función es la **función de similitud Gausiana** $s(x_i, x_j) = \\exp{(-\\frac{\\|x_i - x_j \\|^2}{2\\sigma^2})}$\n",
    "    * $\\sigma$ controla el tamaño de la vecindad  similar a $\\varepsilon$ del grafo  $\\varepsilon-$neigborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grafos Laplacianos\n",
    "\n",
    "Las principales herramientas para Spectral Clustering son las *matrices de grafos Laplacianos*.\n",
    "\n",
    "Asumiendo que $G$ es un grado no dirigido y ponderado cuya matriz de adyacencia esta dada por $W$ donde $w_{ij} = w_{ji} \\geq 0$.\n",
    "\n",
    "\n",
    "### 2.1 Grafo Laplaciano No-normalizado\n",
    "La matriz del Grafo Laplaciano no-normalizada se define:\n",
    "\n",
    "$$L = D - A$$\n",
    "\n",
    "Esta satisface las siguientes propiedades:\n",
    "\n",
    "* Para cada vector $f$ en $\\Re^2$ se tiene $$f L f' = \\frac{1}{2} \\sum_{i,j=1}^{n}{w_{ij}(f_i-f_j)^2}$$\n",
    "* L es simétrica y semi-definida positiva.\n",
    "* El eigenvalor más pequeño de $L$ es $0$.\n",
    "* $L$ tiene n no-negativos y reales eugenvalores $0 = \\lambda_1 \\leq \\lambda_2 \\leq ... \\leq \\lambda_n$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Número de Componentes Conexas y Spectrum de L\n",
    "\n",
    "Dado un grafo no dirigido $G$ con pesos no negativos.\n",
    "\n",
    "* La multiplicidad de $k$ del eigenvalor $0$ de $L$, es igual al número de componentes conexas $A_1, A_2, ...,A_k$ del grafo. \n",
    "\n",
    "\n",
    "###### Ejemplo:\n",
    "En el caso de $k = 1$, lo cual indica que el grafo es conexo. Entonces asumimos que $f$ es un eigenvalor con un eigenvalor igual a $0$.\n",
    "$$ 0 = f L f' = \\frac{1}{2} \\sum_{i,j=1}^{n}{w_{ij}(f_i-f_j)^2}$$\n",
    "\n",
    "* Dos vértices estan conectados si $w_{ij} >  0$.\n",
    "* La ecuación anterior se cumple si $f_i - f_j = 0 $, es decir, ambos sean iguales. Por ende, el $f$ tiene que ser constante en todo el componente conexo. Entonces se tiene un sólo eigenvector con su eigenvalor igual a $0$.\n",
    "\n",
    "\n",
    "\n",
    "Ahora consederemos que se tiene $k-componentes$ conexas, de forma mas general asumamos que los vértices estan ordenados y agrupados de acuerdo a sus componentes conexas. En este caso la matriz $W$ queda dividida como sigue:\n",
    " $$L = \\begin{pmatrix}\n",
    "L_1 & &  &  \\\\\n",
    " & L_2 &  &  \\\\\n",
    "&  & \\ddots & \\\\\n",
    "&  &  & L_k \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Cada bloque $L_i$ es de por si también un grafo Laplaciano, por lo cual es el Laplaciano correspondiente al $i-esimo$ componente conexo de $L$.\n",
    "\n",
    "En conclusión el Spectrum de $L$ esta dado por la unión de los espectros $L_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grafo Laplaciano Normalizado\n",
    "\n",
    "La Matriz Laplaciana puede ser definida por:\n",
    "$$L_{norm(i,j)} = \n",
    "\\begin{cases}\n",
    "1 & \\text{, si $i = j$ y $d_i$ $\\neq$ 0}\\\\\n",
    "-\\frac{1}{\\sqrt{d_i *d_j}} & \\text{, si  $i \\neq j$ y $v_i$ es adyacente a $v_j$}\\\\\n",
    "0 & \\text{, otro caso.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "La matriz Laplaciana Normalizada $L_{norm}$ y la matriz Laplaciana $L$ están relacionadas como sigue:\n",
    "\n",
    "$$L_{norm} := D^{-1/2}LD^{-1/2}$$\n",
    "\n",
    "Esta matriz satisface las siguientes propiedades:\n",
    "\n",
    "* Para cada vector $f$ en $\\Re^2$ se tiene $$f L_{norm} f' = \\frac{1}{2} \\sum_{i,j=1}^{n}{w_{ij}(\\frac{f_i}{\\sqrt{d_i}} -\\frac{f_j}{\\sqrt{d_j}})^2}$$\n",
    "* L es simétrica y semi-definida positiva.\n",
    "* El eigenvalor más pequeño de $L$ es $0$.\n",
    "* $L_{norm}$ tiene n no-negativos y reales eugenvalores $0 = \\lambda_1 \\leq \\lambda_2 \\leq ... \\leq \\lambda_n$ \n",
    "\n",
    "Tanto en la matriz $L$ como en la matriz $L_{norm}$, la multiplicidad $k$ del eigenvalor $0$ indican el número de componente conexas $A_i, A_2,...,A_k$ del grafo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algoritmos de Spectral Clustering\n",
    "\n",
    "* Asumimos que nuestros datos son $n$ \"puntos\" $x_1, x_2,...,x_n$ los cuales pueden ser arbitrariamente objetos.\n",
    "* Se cuenta con un valor de similitud entre cada par $s_{ij} = s(x_i, x_j)$ mediante una función de similitus que sea  simétrica y no negativa. Como resultado se obtiene la *Matriz de Similitud* $S=(s_{ij})_{i,j=1,...,n}$\n",
    "\n",
    "### 3.1 Spectral Clustering No-Normalizado\n",
    "Dado una matriz $S \\in \\Re^{nxn}$ y un número $k$ con clusters a construir:\n",
    "\n",
    "* Construimos un *Grafo de Similitud* contenida en la matriz de adyacencia ponderada $W$,  mediante los métodos mencionados anteriormente.\n",
    "* Cálculo de la matriz Laplaciana no normalizada $L$ de W.\n",
    "* Cálculo de los primero $k$ eigenvectores $u_1, u_2,...,u_k$ de $L$.\n",
    "* Dado $U \\in \\Re^{nxk}$, una matriz que contiene todos los eigenvectores como columnas.\n",
    "* Sea $y_{i=1,...,n} \\in \\Re^k$, un vector pertenceciente a $U$. Agrupar los puntos $y_i$ con el algoritmo **k-means** dentro de $C_1,...,C_k$ grupos.\n",
    "    \n",
    "Obtenemos los grupos $A_1,...,A_k$ con $A_i = \\{j | y_j \\in C_i\\}$\n",
    "\n",
    "### 3.2 Spectral Clustering Normalizado\n",
    "\n",
    "Dado una matriz $S \\in \\Re^{nxn}$ y un número $k$ con clusters a construir:\n",
    "\n",
    "* Construimos un *Grafo de Similitud* contenida en la matriz de adyacencia ponderada $W$,  mediante los métodos mencionados anteriormente.\n",
    "* Cálculo de la matriz Laplaciana  normalizada $L_{norm}$ de W.\n",
    "* Cálculo de los primero $k$ eigenvectores $u_1, u_2,...,u_k$ de $L_{norm}$.\n",
    "* Dado $U \\in \\Re^{nxk}$, una matriz que contiene todos los eigenvectores como columnas.\n",
    "* Obtenemos la Matriz $T \\in \\Re^{nxk}$ basada en  la normalización de $U$.\n",
    "$$t_{ij} = \\frac{u_{ij}}{\\sqrt{\\sum_{k}{u_{ik}^2}}}$$\n",
    "* Sea $y_{i=1,...,n} \\in \\Re^k$, un vector pertenceciente a $T$. Agrupar los puntos $y_i$ con el algoritmo **k-means** dentro de $C_1,...,C_k$ grupos.\n",
    "    \n",
    "Obtenemos los grupos $A_1,...,A_k$ con $A_i = \\{j | y_j \\in C_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
